{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wer\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import observation_model\n",
    "import openfst_python as fst\n",
    "import matplotlib.pyplot as plt\n",
    "from subprocess import check_call\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "#===============================================================================\n",
    "# ============================== UTILITY FUNCTIONS =============================\n",
    "#===============================================================================\n",
    "\n",
    "def read_transcription(wav_file):\n",
    "    \"\"\"\n",
    "    Get the transcription corresponding to wav_file.\n",
    "    \"\"\"\n",
    "    \n",
    "    transcription_file = os.path.splitext(wav_file)[0] + '.txt'\n",
    "    \n",
    "    with open(transcription_file, 'r') as f:\n",
    "        transcription = f.readline().strip()\n",
    "    \n",
    "    return transcription\n",
    "\n",
    "\n",
    "def get_unigram_and_bigram_probs():\n",
    "    i = 0\n",
    "    train_split = int(0.5 * len(glob.glob('/group/teaching/asr/labs/recordings/*.wav')))  \n",
    "\n",
    "\n",
    "    all_transcriptions = ''\n",
    "    for wav_file in glob.glob('/group/teaching/asr/labs/recordings/*.wav'):   \n",
    "\n",
    "        transcription = read_transcription(wav_file)\n",
    "        all_transcriptions += transcription + ' \\n'\n",
    "\n",
    "        i += 1\n",
    "        if i == train_split:\n",
    "            break\n",
    "\n",
    "    # count unigram counts in all_transcriptions\n",
    "    unigram_counts = {}\n",
    "    for word in all_transcriptions.replace(\"\\n\", \" \").split():\n",
    "        if word in unigram_counts:\n",
    "            unigram_counts[word] += 1\n",
    "        else:\n",
    "            unigram_counts[word] = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    unigram_probs = {}\n",
    "    for word, count in unigram_counts.items():\n",
    "        unigram_probs[word] = count / sum(unigram_counts.values())\n",
    "\n",
    "\n",
    "    # save unigram probs to pickle file\n",
    "    import pickle\n",
    "    with open('unigram_probs.pickle', 'wb') as handle:\n",
    "        pickle.dump(unigram_probs, handle)\n",
    "        # load unigram probs from pickle file\n",
    "    import pickle\n",
    "    with open('unigram_probs.pickle', 'rb') as handle:\n",
    "        unigram_probs = pickle.load(handle)\n",
    "\n",
    "    # count unigram counts in all_transcriptions\n",
    "    bigram_counts = {}\n",
    "    for line in all_transcriptions.split(\"\\n\"):\n",
    "        line = line.split()\n",
    "        for idx, word in enumerate(line):\n",
    "            if idx > 0 and (line[idx - 1], word) in bigram_counts:\n",
    "                bigram_counts[(line[idx - 1], word)] += 1\n",
    "            elif idx == 0 and (\"<start>\", word) in bigram_counts:\n",
    "                bigram_counts[(\"<start>\", word)] += 1\n",
    "            elif idx == 0 and (\"<start>\", word) not in bigram_counts:\n",
    "                bigram_counts[(\"<start>\", word)] = 1\n",
    "            else:\n",
    "                bigram_counts[(line[idx - 1], word)] = 1\n",
    "\n",
    "    # calculate bigram probabilities\n",
    "    bigram_probs = {}\n",
    "    for bigram, count in bigram_counts.items():\n",
    "        if bigram[0] == \"<start>\":\n",
    "            bigram_probs[bigram] = count / len(all_transcriptions.split(\"\\n\"))\n",
    "        else:\n",
    "            bigram_probs[bigram] = count / unigram_counts[bigram[0]]\n",
    "\n",
    "    return unigram_probs, bigram_probs\n",
    "\n",
    "\n",
    "def draw(f):\n",
    "    f.draw('tmp.dot', portrait=True)\n",
    "    check_call(['dot','-Tpng','-Gdpi=1800','tmp.dot','-o','tmp.png'])\n",
    "    return Image(filename='tmp.png')\n",
    "\n",
    "\n",
    "def parse_lexicon(lex_file):\n",
    "    \"\"\"\n",
    "    Parse the lexicon file and return it in dictionary form.\n",
    "    \n",
    "    Args:\n",
    "        lex_file (str): filename of lexicon file with structure '<word> <phone1> <phone2>...'\n",
    "                        eg. peppers p eh p er z\n",
    "\n",
    "    Returns:\n",
    "        lex (dict): dictionary mapping words to list of phones\n",
    "    \"\"\"\n",
    "    \n",
    "    lex = {}  # create a dictionary for the lexicon entries (this could be a problem with larger lexica)\n",
    "    with open(lex_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.split()  # split at each space\n",
    "            lex[line[0]] = line[1:]  # first field the word, the rest is the phones\n",
    "    return lex\n",
    "\n",
    "\n",
    "def generate_symbol_tables(lexicon, n=3, with_silence=True):\n",
    "    '''\n",
    "    Return word, phone and state symbol tables based on the supplied lexicon\n",
    "        \n",
    "    Args:\n",
    "        lexicon (dict): lexicon to use, created from the parse_lexicon() function\n",
    "        n (int): number of states for each phone HMM\n",
    "        \n",
    "    Returns:\n",
    "        word_table (fst.SymbolTable): table of words\n",
    "        phone_table (fst.SymbolTable): table of phones\n",
    "        state_table (fst.SymbolTable): table of HMM phone-state IDs\n",
    "    '''\n",
    "    \n",
    "    state_table = fst.SymbolTable()\n",
    "    phone_table = fst.SymbolTable()\n",
    "    word_table = fst.SymbolTable()\n",
    "    \n",
    "    # add empty <eps> symbol to all tables\n",
    "    state_table.add_symbol('<eps>')\n",
    "    phone_table.add_symbol('<eps>')\n",
    "    word_table.add_symbol('<eps>')\n",
    "    \n",
    "    for word, phones  in lexicon.items():\n",
    "        \n",
    "        word_table.add_symbol(word)\n",
    "        \n",
    "        for p in phones: # for each phone\n",
    "            \n",
    "            phone_table.add_symbol(p)\n",
    "            for i in range(1,n+1): # for each state 1 to n\n",
    "                state_table.add_symbol('{}_{}'.format(p, i))\n",
    "\n",
    "    if with_silence:\n",
    "        # add silence symbols\n",
    "        phone_table.add_symbol('sil')\n",
    "        for i in range(1,n+1):\n",
    "            state_table.add_symbol('sil_{}'.format(i))\n",
    "            \n",
    "    return word_table, phone_table, state_table\n",
    "\n",
    "#===============================================================================\n",
    "# ============================== VITERBI DECODING ==============================\n",
    "#===============================================================================\n",
    "\n",
    "class MyViterbiDecoder:\n",
    "    \n",
    "    NLL_ZERO = 1e10 \n",
    "    \n",
    "    def __init__(self, f, audio_file_name, verbose=False, use_pruning=False, determinized=False, bigram = False, histogram_pruning_threshold = 0):\n",
    "        \"\"\"Set up the decoder class with an audio file and WFST f\n",
    "        \"\"\"\n",
    "        self.lex = parse_lexicon('lexicon.txt')\n",
    "        self.verbose = verbose\n",
    "        self.om = observation_model.ObservationModel()\n",
    "        self.f = f\n",
    "        self.number_of_computiations = 0\n",
    "        self.decode_time = 0\n",
    "        self.backtrace_time = 0\n",
    "        self.use_pruning = use_pruning\n",
    "        self.prune_threshold = self.NLL_ZERO\n",
    "        self.determinized = determinized\n",
    "        self.bigram = bigram\n",
    "        self.word_start = -1\n",
    "        self.histogram_pruning_threshold = histogram_pruning_threshold\n",
    "\n",
    "\n",
    "        for state in self.f.states():\n",
    "            for arc in self.f.arcs(state):\n",
    "                o = self.f.output_symbols().find(arc.olabel)\n",
    "                if o =='sil':\n",
    "                    self.word_start = arc.nextstate\n",
    "\n",
    "        if self.word_start == -1:\n",
    "            if self.determinized:\n",
    "                self.word_start = 1\n",
    "                   \n",
    "                    \n",
    "\n",
    "        if self.use_pruning:\n",
    "            self.prune_threshold = 30.0\n",
    "        \n",
    "        if audio_file_name:\n",
    "            self.om.load_audio(audio_file_name)\n",
    "        else:\n",
    "            self.om.load_dummy_audio()\n",
    "        \n",
    "        self.initialise_decoding()\n",
    "\n",
    "        \n",
    "    def initialise_decoding(self):\n",
    "        \"\"\"set up the values for V_j(0) (as negative log-likelihoods)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.V = []   # stores likelihood along best path reaching state j\n",
    "        self.B = []   # stores identity of best previous state reaching state j\n",
    "        self.W = []   # stores output labels sequence along arc reaching j - this removes need for \n",
    "                      # extra code to read the output sequence along the best path\n",
    "        \n",
    "        for t in range(self.om.observation_length()+1):\n",
    "            self.V.append([self.NLL_ZERO]*self.f.num_states())\n",
    "            self.B.append([-1]*self.f.num_states())\n",
    "            self.W.append([[] for i in range(self.f.num_states())])  #  multiplying the empty list doesn't make multiple\n",
    "        \n",
    "        # The above code means that self.V[t][j] for t = 0, ... T gives the Viterbi cost\n",
    "        # of state j, time t (in negative log-likelihood form)\n",
    "        # Initialising the costs to NLL_ZERO effectively means zero probability    \n",
    "        \n",
    "        # give the WFST start state a probability of 1.0   (NLL = 0.0)\n",
    "        self.V[0][self.f.start()] = 0.0\n",
    "        \n",
    "        # some WFSTs might have arcs with epsilon on the input (you might have already created \n",
    "        # examples of these in earlier labs) these correspond to non-emitting states, \n",
    "        # which means that we need to process them without stepping forward in time.  \n",
    "        # Don't worry too much about this!  \n",
    "        self.traverse_epsilon_arcs(0)        \n",
    "\n",
    "\n",
    "    def traverse_epsilon_arcs(self, t):\n",
    "        \"\"\"Traverse arcs with <eps> on the input at time t\n",
    "        \n",
    "        These correspond to transitions that don't emit an observation\n",
    "        \n",
    "        We've implemented this function for you as it's slightly trickier than\n",
    "        the normal case.  You might like to look at it to see what's going on, but\n",
    "        don't worry if you can't fully follow it.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        states_to_traverse = list(self.f.states()) # traverse all states\n",
    "        while states_to_traverse:\n",
    "            \n",
    "            # Set i to the ID of the current state, the first \n",
    "            # item in the list (and remove it from the list)\n",
    "            i = states_to_traverse.pop(0)   \n",
    "        \n",
    "            # don't bother traversing states which have zero probability\n",
    "            if self.V[t][i] == self.NLL_ZERO:\n",
    "                    continue\n",
    "        \n",
    "            for arc in self.f.arcs(i):\n",
    "                \n",
    "                if arc.ilabel == 0:     # if <eps> transition\n",
    "                  \n",
    "                    j = arc.nextstate   # ID of next state  \n",
    "                \n",
    "                    if self.V[t][j] > self.V[t][i] + float(arc.weight):\n",
    "                        \n",
    "                        # this means we've found a lower-cost path to\n",
    "                        # state j at time t.  We might need to add it\n",
    "                        # back to the processing queue.\n",
    "                        self.V[t][j] = self.V[t][i] + float(arc.weight)\n",
    "                        \n",
    "                        # save backtrace information.  In the case of an epsilon transition, \n",
    "                        # we save the identity of the best state at t-1.  This means we may not\n",
    "                        # be able to fully recover the best path, but to do otherwise would\n",
    "                        # require a more complicated way of storing backtrace information\n",
    "                        self.B[t][j] = self.B[t][i] \n",
    "                        \n",
    "                        # and save the output labels encountered - this is a list, because\n",
    "                        # there could be multiple output labels (in the case of <eps> arcs)\n",
    "                        if arc.olabel != 0:\n",
    "                            self.W[t][j] = self.W[t][i] + [arc.olabel]\n",
    "                        else:\n",
    "                            self.W[t][j] = self.W[t][i]\n",
    "                        \n",
    "                        if j not in states_to_traverse:\n",
    "                            states_to_traverse.append(j)\n",
    "\n",
    "\n",
    "    def forward_step(self, t):\n",
    "        '''\n",
    "        Propagate the Viterbi costs forward one time step\n",
    "        '''\n",
    "        for i in self.f.states():\n",
    "            \n",
    "            if not self.V[t-1][i] == self.NLL_ZERO:   # no point in propagating states with zero probability\n",
    "                \n",
    "                for arc in self.f.arcs(i):\n",
    "                    \n",
    "                    \n",
    "                    if arc.ilabel != 0: # <eps> transitions don't emit an observation\n",
    "                        self.number_of_computiations += 1\n",
    "                        j = arc.nextstate\n",
    "                        tp = float(arc.weight)  # transition prob\n",
    "                        ep = -self.om.log_observation_probability(self.f.input_symbols().find(arc.ilabel), t)  # emission negative log prob\n",
    "\n",
    "\n",
    "                        prob = tp + ep + self.V[t-1][i] # they're logs\n",
    "\n",
    "                        \n",
    "\n",
    "                        if prob < self.V[t][j]:\n",
    "                            self.V[t][j] = prob\n",
    "                            self.B[t][j] = i\n",
    "                            \n",
    "                            # store the output labels encountered too\n",
    "                            if arc.olabel !=0:\n",
    "                                self.W[t][j] = [arc.olabel]\n",
    "                            else:\n",
    "                                self.W[t][j] = []\n",
    "\n",
    "        if self.use_pruning and self.histogram_pruning_threshold == 0:    \n",
    "            best_path = min(self.V[t])\n",
    "            for idx, path in enumerate(self.V[t]):\n",
    "                if path - best_path > self.prune_threshold:\n",
    "                    self.V[t][idx] = self.NLL_ZERO\n",
    "\n",
    "        if self.use_pruning and self.histogram_pruning_threshold > 0:\n",
    "            # get the indices and values of paths in V[t] that do not have NLL_ZERO probability\n",
    "            indices = [i for i, x in enumerate(self.V[t]) if x != self.NLL_ZERO]\n",
    "            values = [x for x in self.V[t] if x != self.NLL_ZERO]\n",
    "\n",
    "            # check if there are more than histogram_threshold paths\n",
    "            if len(values) > self.histogram_pruning_threshold:\n",
    "                # keep the histogram_threshold best paths and set the rest to NLL_ZERO\n",
    "                best_paths = np.argpartition(values, self.histogram_pruning_threshold)[:self.histogram_pruning_threshold]\n",
    "                for idx in indices:\n",
    "                    if idx not in best_paths:\n",
    "                        self.V[t][idx] = self.NLL_ZERO\n",
    "            \n",
    "                        \n",
    "    def finalise_decoding(self):\n",
    "        \"\"\" this incorporates the probability of terminating at each state\n",
    "        \"\"\"\n",
    "        \n",
    "        for state in self.f.states():\n",
    "            final_weight = float(self.f.final(state))\n",
    "            if self.V[-1][state] != self.NLL_ZERO:\n",
    "                if final_weight == math.inf:\n",
    "                    self.V[-1][state] = self.NLL_ZERO  # effectively says that we can't end in this state\n",
    "                else:\n",
    "                    self.V[-1][state] += final_weight\n",
    "                    \n",
    "        # get a list of all states where there was a path ending with non-zero probability\n",
    "        finished = [x for x in self.V[-1] if x < self.NLL_ZERO]\n",
    "        if not finished and self.verbose:  # if empty\n",
    "            print(\"No path got to the end of the observations.\")\n",
    "        \n",
    "        \n",
    "    def decode(self):\n",
    "        start = time.time()\n",
    "        self.initialise_decoding()\n",
    "        t = 1\n",
    "        while t <= self.om.observation_length():\n",
    "            self.forward_step(t)\n",
    "            self.traverse_epsilon_arcs(t)\n",
    "            t += 1\n",
    "            \n",
    "        self.finalise_decoding()\n",
    "        end = time.time()\n",
    "        self.decode_time = end-start\n",
    "        if self.verbose:\n",
    "            print(\"Decoding took\", self.decode_time,  \"seconds\")\n",
    "            print(\"Number of computations:\", self.number_of_computiations)\n",
    "\n",
    "    \n",
    "    def backtrace(self):\n",
    "        start = time.time()\n",
    "        best_final_state = self.V[-1].index(min(self.V[-1])) # argmin\n",
    "        best_state_sequence = [best_final_state]\n",
    "        best_out_sequence = []\n",
    "        \n",
    "        t = self.om.observation_length()   # ie T\n",
    "        j = best_final_state\n",
    "\n",
    "\n",
    "        next_i = -1\n",
    "        prev_i = -1\n",
    "        # self.determinized = False\n",
    "        while t >= 0:\n",
    "            i = self.B[t][j]\n",
    "            best_state_sequence.append(i)\n",
    "\n",
    "            if self.determinized:\n",
    "\n",
    "                if i == self.word_start  and self.determinized:\n",
    "                    best_out_sequence = self.W[t][j] + [0] + best_out_sequence\n",
    "                else: \n",
    "                    best_out_sequence = self.W[t][j] + best_out_sequence                              \n",
    "\n",
    "\n",
    "\n",
    "            else:\n",
    "                is_final =  False\n",
    "                \n",
    "                if i >=0:\n",
    "                    for arc in self.f.arcs(i):\n",
    "                        next_i = arc.nextstate\n",
    "                        if next_i != i and i != prev_i:\n",
    "                            if float(self.f.final(next_i)) != math.inf:\n",
    "                                best_out_sequence = self.W[t][j] + [0] + best_out_sequence\n",
    "                                is_final = True\n",
    "                    if not is_final:\n",
    "                        best_out_sequence = self.W[t][j] + best_out_sequence\n",
    "\n",
    "                    prev_i = i\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            # continue the backtrace at state i, time t-1\n",
    "            j = i  \n",
    "            t-=1\n",
    "            \n",
    "        best_state_sequence.reverse()\n",
    "        \n",
    "        \n",
    "        best_out_sequence = ' '.join([ self.f.output_symbols().find(label) for label in best_out_sequence])\n",
    "\n",
    "        if self.bigram or self.determinized:\n",
    "            split_word = \"sil\"\n",
    "        else:\n",
    "            split_word = \"<eps>\"\n",
    "\n",
    "        phones = best_out_sequence.split(split_word)\n",
    "        phones[0] = phones[0].replace(\"sil\",\"\")\n",
    "        # print('Phones: ', phones)\n",
    "        # print('best_out_sequence: ', best_out_sequence)\n",
    "        # print('best_state_sequence: ', best_state_sequence)\n",
    "        words = \"\"\n",
    "      \n",
    "        for phone in phones:\n",
    "            phone = phone.replace(' ', '')\n",
    "            for key, val in self.lex.items():\n",
    "                val = ''.join(val)\n",
    "                if val == phone:\n",
    "                    words += key + \" \"\n",
    "\n",
    "\n",
    "        end = time.time()\n",
    "        self.backtrace_time = end-start\n",
    "        if self.verbose:\n",
    "            print(\"Backtrace took\", self.backtrace_time, \"seconds\")\n",
    "        \n",
    "        return (best_state_sequence, words)\n",
    "\n",
    "\n",
    "#===================================================================================================\n",
    "#======================================  HMM  ======================================================\n",
    "#===================================================================================================\n",
    "\n",
    "\n",
    "unigram_probs, bigram_probs = get_unigram_and_bigram_probs()\n",
    "lex = parse_lexicon('lexicon.txt')\n",
    "word_table, phone_table, state_table = generate_symbol_tables(lex)\n",
    "\n",
    "\n",
    "def generate_phone_wfst(f, start_state, phone, n, sl_prop=0.1, tl_prop=0.9):\n",
    "    \"\"\"\n",
    "    Generate a WFST representating an n-state left-to-right phone HMM\n",
    "    \n",
    "    Args:\n",
    "        f (fst.Fst()): an FST object, assumed to exist already\n",
    "        start_state (int): the index of the first state, assmed to exist already\n",
    "        phone (str): the phone label \n",
    "        n (int): number of states for each phone HMM\n",
    "        \n",
    "    Returns:\n",
    "        the final state of the FST\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    current_state = start_state\n",
    "    \n",
    "    for i in range(1, n+1):\n",
    "        \n",
    "        in_label = state_table.find('{}_{}'.format(phone, i))\n",
    "        \n",
    "        sl_weight = fst.Weight('tropical', -math.log(sl_prop))  # weight for self-loop\n",
    "        # self-loop back to current state\n",
    "        f.add_arc(current_state, fst.Arc(in_label, 0, sl_weight, current_state))\n",
    "        \n",
    "        # transition to next state\n",
    "        \n",
    "        # we want to output the phone label on the final state\n",
    "        # note: if outputting words instead this code should be modified\n",
    "        if i == n:\n",
    "            out_label = phone_table.find(phone)\n",
    "        else:\n",
    "            out_label = 0   # output empty <eps> label\n",
    "            \n",
    "        next_state = f.add_state()\n",
    "        next_weight = fst.Weight('tropical', -math.log(tl_prop)) # weight to next state\n",
    "        f.add_arc(current_state, fst.Arc(in_label, out_label, next_weight, next_state))    \n",
    "       \n",
    "        cp = current_state\n",
    "        current_state = next_state\n",
    "\n",
    "    return current_state\n",
    "\n",
    "\n",
    "def generate_word_sequence_recognition_wfst(n, sl_prop=0.1, tl_prop=0.9):\n",
    "    \"\"\" generate a HMM to recognise any single word sequence for words in the lexicon\n",
    "    \n",
    "    Args:\n",
    "        n (int): states per phone HMM\n",
    "\n",
    "    Returns:\n",
    "        the constructed WFST\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    f = fst.Fst()\n",
    "    \n",
    "    # create a single start state\n",
    "    start_state = f.add_state()\n",
    "    f.set_start(start_state)\n",
    "    \n",
    "    for word, phones in lex.items():\n",
    "        current_state = f.add_state()\n",
    "        f.add_arc(start_state, fst.Arc(0, 0, None, current_state))\n",
    "        \n",
    "        for phone in phones: \n",
    "            current_state = generate_phone_wfst(f, current_state, phone, n, sl_prop, tl_prop)\n",
    "        # note: new current_state is now set to the final state of the previous phone WFST\n",
    "        \n",
    "        f.set_final(current_state)\n",
    "        f.add_arc(current_state, fst.Arc(0, 0, None, start_state))\n",
    "        \n",
    "    return f\n",
    "\n",
    "\n",
    "def generate_word_sequence_recognition_wfst_with_silence(n, use_unigram_probs=False, sl_prop=0.1, tl_prop=0.9):\n",
    "    \"\"\" generate a HMM to recognise any single word sequence for words in the lexicon with 2 silence states at the beginning and end\n",
    "    \n",
    "    Args:\n",
    "        n (int): states per phone HMM\n",
    "\n",
    "    Returns:\n",
    "        the constructed WFST\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    f = fst.Fst()\n",
    "\n",
    "    # create a single start state\n",
    "    start_state = f.add_state()\n",
    "    f.set_start(start_state)\n",
    "\n",
    "    silence_state = generate_phone_wfst(f, start_state, 'sil', 3, sl_prop, tl_prop)\n",
    "    i = 0\n",
    "    for word, phones in lex.items():\n",
    "        \n",
    "        \n",
    "        current_state = f.add_state()\n",
    "        if use_unigram_probs:\n",
    "            unigram_weight = fst.Weight('tropical', -math.log(unigram_probs[word]))\n",
    "        else:\n",
    "            unigram_weight = None\n",
    "        f.add_arc(silence_state, fst.Arc(0, 0, unigram_weight , current_state))\n",
    "\n",
    "        for index, phone in enumerate(phones):\n",
    "\n",
    "            current_state = generate_phone_wfst(f, current_state, phone, n, sl_prop, tl_prop)\n",
    "        # note: new current_state is now set to the final state of the previous phone WFST\n",
    "\n",
    "        f.set_final(current_state)\n",
    "        silence_state2 = generate_phone_wfst(f, current_state, 'sil', 1, sl_prop, tl_prop)\n",
    "        f.add_arc(silence_state2, fst.Arc(0, 0, None, silence_state))\n",
    "\n",
    "\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def generate_bigram_wfst(n):\n",
    "    \"\"\" generate a HMM to recognise any single word sequence for words in the lexicon with 2 silence states at the beginning and end\n",
    "    \n",
    "    Args:\n",
    "        n (int): states per phone HMM\n",
    "\n",
    "    Returns:\n",
    "        the constructed WFST\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    f = fst.Fst()\n",
    "\n",
    "    # create a single start state\n",
    "    start_state = f.add_state()\n",
    "    f.set_start(start_state)\n",
    "    silence_state = generate_phone_wfst(f, start_state, 'sil', 3)\n",
    "\n",
    "\n",
    "\n",
    "    word_states_dict = {}\n",
    "    for word, phones in lex.items():\n",
    "        word_state = f.add_state()\n",
    "        bigram_start_weight = fst.Weight('tropical', 1e10)\n",
    "        if ('<start>', word) in bigram_probs:\n",
    "            bigram_start_weight = fst.Weight('tropical', -math.log(bigram_probs[('<start>',word)]))\n",
    "\n",
    "        f.add_arc(silence_state, fst.Arc(0, 0, bigram_start_weight, word_state))\n",
    "        word_states_dict[word] = word_state\n",
    "        f.set_final(word_state)\n",
    "        \n",
    "    for key, value in word_states_dict.items():\n",
    "        for word, phones in lex.items():\n",
    "            current_state = f.add_state()\n",
    "\n",
    "\n",
    "            bigram_weight = fst.Weight('tropical', 1e10)\n",
    "            \n",
    "            if (key, word)  in bigram_probs:\n",
    "                bigram_weight = fst.Weight('tropical', -math.log(bigram_probs[(key,word)]))\n",
    "\n",
    "            f.add_arc(value, fst.Arc(0, 0, bigram_weight, current_state))\n",
    "            for index, phone in enumerate(phones):\n",
    "\n",
    "                current_state = generate_phone_wfst(f, current_state, phone, n)\n",
    "            \n",
    "            silence_state = generate_phone_wfst(f, current_state, 'sil', 3)\n",
    "\n",
    "            f.add_arc(silence_state, fst.Arc(0, 0, None, word_states_dict[word]))\n",
    "    return f\n",
    "\n",
    "\n",
    "#==============================================================================\n",
    "#========================== Set up the Experiment =============================\n",
    "#==============================================================================\n",
    "\n",
    "def create_wfst(n):\n",
    "    f = generate_word_sequence_recognition_wfst(n)\n",
    "    f.set_input_symbols(state_table)\n",
    "    f.set_output_symbols(phone_table)\n",
    "    return f\n",
    "\n",
    "\n",
    "def create_tuned_wfst(n, use_unigram_probs=True):\n",
    "    f = generate_word_sequence_recognition_wfst_with_silence(n,use_unigram_probs=use_unigram_probs, sl_prop=0.95, tl_prop=0.05)\n",
    "    f.set_input_symbols(state_table)\n",
    "    f.set_output_symbols(phone_table)\n",
    "    return f\n",
    "\n",
    "\n",
    "def create_wfst_with_silence(n, use_unigram_probs=False):\n",
    "    f = generate_word_sequence_recognition_wfst_with_silence(n, use_unigram_probs=use_unigram_probs)\n",
    "    f.set_input_symbols(state_table)\n",
    "    f.set_output_symbols(phone_table)\n",
    "    return f\n",
    "\n",
    "\n",
    "def create_bigram_lexical(n):\n",
    "    f = generate_bigram_wfst(n)\n",
    "    f.set_input_symbols(state_table)\n",
    "    f.set_output_symbols(phone_table)\n",
    "    return f\n",
    "\n",
    "\n",
    "def memory_of_wfst(f):\n",
    "    '''\n",
    "    Compute a measure of the memory required for your decoder by providing counts\n",
    "    of number of states and arcs in the WFST.\n",
    "    '''\n",
    "    all_states = []\n",
    "    all_arcs = []\n",
    "    for state in f.states():\n",
    "        all_states.append(state)\n",
    "        for arc in f.arcs(state):\n",
    "            all_arcs.append(arc)\n",
    "    return len(all_states), len(all_arcs)\n",
    "\n",
    "    \n",
    "def get_avg_wer(all_losses, verbose=False):\n",
    "    all_wer = []\n",
    "    for error_counts, word_count in all_losses:\n",
    "        all_wer.append(sum(error_counts) / word_count)\n",
    "    \n",
    "    if verbose :\n",
    "        print(f'The average WER is {np.mean(all_wer):.2%}')    \n",
    "    return np.mean(all_wer)\n",
    "\n",
    "\n",
    "def get_avg_effciency(efficancy_measures, verbose=False):\n",
    "    decoding_time = np.mean(efficancy_measures[0])\n",
    "    backtrace_time = np.mean(efficancy_measures[1])\n",
    "    number_of_computions = np.mean(efficancy_measures[2])\n",
    "    if verbose:\n",
    "        print(f'The average decoding time is {decoding_time:.2f} seconds')\n",
    "        print(f'The average backtrace time is {backtrace_time:.2f} seconds')\n",
    "        print(f'The average number of computations is {number_of_computions:.2f}')\n",
    "    return decoding_time, backtrace_time, number_of_computions\n",
    "\n",
    "\n",
    "def decoding_loop(f, train_set=True, train_split=0.5, use_pruning=False, determinized=False, verbose=False, prune_threshold= None, bigram = False, histogram_pruning_threshold = 0):\n",
    "    all_losses = []\n",
    "    decoding_time = []\n",
    "    backtrace_time = []\n",
    "    number_of_computations = []\n",
    "    all_files = glob.glob('/group/teaching/asr/labs/recordings/*.wav')\n",
    "    train_files = all_files[:(int(train_split*len(all_files)))]\n",
    "    test_files = all_files[(int(train_split*len(all_files))):]\n",
    "    \n",
    "    if train_set:\n",
    "        files= train_files\n",
    "    else:\n",
    "        files = test_files\n",
    "    \n",
    "    for wav_file in tqdm(files):    \n",
    "        decoder  = MyViterbiDecoder(f, wav_file, verbose=verbose, use_pruning=use_pruning, determinized=determinized, bigram=bigram, histogram_pruning_threshold=histogram_pruning_threshold)\n",
    "        if use_pruning and prune_threshold!=None:\n",
    "            decoder.prune_threshold = prune_threshold\n",
    "        decoder.decode()\n",
    "        (state_path, words) = decoder.backtrace()  \n",
    "        transcription = read_transcription(wav_file)\n",
    "        error_counts = wer.compute_alignment_errors(transcription, words)\n",
    "        word_count = len(transcription.split())\n",
    "\n",
    "        all_losses.append((error_counts, word_count))\n",
    "        decoding_time.append(decoder.decode_time)\n",
    "        backtrace_time.append(decoder.backtrace_time)\n",
    "        number_of_computations.append(decoder.number_of_computiations)\n",
    "        if verbose:\n",
    "            print(f'Transcription: {transcription} || Prediction: {words} || (nsub, ndel, nin) :{error_counts}')\n",
    "    \n",
    "    efficancy_measures = (decoding_time, backtrace_time, number_of_computations)\n",
    "    return all_losses, efficancy_measures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created all the wfst\n"
     ]
    }
   ],
   "source": [
    "lex = parse_lexicon('lexicon.txt')\n",
    "word_table, phone_table, state_table = generate_symbol_tables(lex)\n",
    "n = 3\n",
    "f = create_wfst(n)\n",
    "f_silence = create_wfst_with_silence(n)\n",
    "f_bigram = create_bigram_lexical(n)\n",
    "f_det = fst.determinize(f)\n",
    "f_silence_det = fst.determinize(f_silence)\n",
    "f_bigram_det = fst.determinize(f_bigram)\n",
    "f_det_min = f_det.copy()\n",
    "f_det_min.minimize(allow_nondet = True)\n",
    "f_silence_det_min = f_silence_det.copy()\n",
    "f_silence_det_min.minimize(allow_nondet = True)\n",
    "f_bigram_det_min = f_bigram_det.copy()\n",
    "f_bigram_det_min.minimize(allow_nondet = True)\n",
    "f_tuned = create_tuned_wfst(n)\n",
    "f_tuned_det = fst.determinize(f_tuned)\n",
    "f_tuned_det_min = f_tuned_det.copy()\n",
    "f_tuned_det_min.minimize(allow_nondet = True)\n",
    "\n",
    "f_tuned_unigram = create_tuned_wfst(n, use_unigram_probs=True)\n",
    "f_tuned_det_unigram = fst.determinize(f_tuned_unigram)\n",
    "f_tuned_det_min_unigram = f_tuned_det.copy()\n",
    "f_tuned_det_min_unigram.minimize(allow_nondet = True)\n",
    "print('Created all the wfst')\n",
    "\n",
    "def run_exp(exp_dict ,wfst_type, det, train_set = True ,train_split = 0.5, verbose=False, use_pruning=False, prune_threshold=None, histogram_pruning_threshold=0, minimized=False):\n",
    "    wfsts = {'baseline': f, 'silence': f_silence, 'bigram': f_bigram, 'tuned': f_tuned, 'tuned_unigram': f_tuned_unigram}\n",
    "    wfsts_det = {'baseline': f_det, 'silence': f_silence_det, 'bigram': f_bigram_det, 'tuned': f_tuned_det, 'tuned_unigram': f_tuned_det_unigram}\n",
    "    wfsts_det_min = {'baseline': f_det_min, 'silence': f_silence_det_min, 'bigram': f_bigram_det_min, 'tuned': f_tuned_det_min, 'tuned_unigram': f_tuned_det_min_unigram}\n",
    "\n",
    "    if minimized:\n",
    "        wfst = wfsts_det_min[wfst_type]\n",
    "    elif det:\n",
    "        wfst = wfsts_det[wfst_type]\n",
    "    else:\n",
    "        wfst = wfsts[wfst_type]\n",
    "    bigram = False\n",
    "    if wfst_type == 'bigram':\n",
    "        bigram = True\n",
    "    \n",
    "    exp_dict['type'] = wfst_type\n",
    "    exp_dict['det'] = det\n",
    "    exp_dict['all_states'], exp_dict['all_arcs'] = memory_of_wfst(wfst)\n",
    "\n",
    "    print(f'Type {wfst_type}, det = {det}, minimized = {minimized}')\n",
    "    print(f'All states: {exp_dict[\"all_states\"]}, all arcs: {exp_dict[\"all_arcs\"]}')\n",
    "\n",
    "\n",
    "    all_losses, efficancy_measures = decoding_loop(wfst, train_set=train_set, train_split=train_split, determinized=det, verbose=verbose, bigram=bigram, use_pruning=use_pruning, prune_threshold=prune_threshold, histogram_pruning_threshold=histogram_pruning_threshold)\n",
    "    avg_wer = get_avg_wer(all_losses, verbose=True)\n",
    "    m1,m2,m3 = get_avg_effciency(efficancy_measures, verbose=verbose)\n",
    "    exp_dict['loss'].append(all_losses)\n",
    "    exp_dict['efficancy'].append(efficancy_measures)\n",
    "    exp_dict['acc'].append(avg_wer)\n",
    "    exp_dict['m1'].append(m1)\n",
    "    exp_dict['m2'].append(m2)\n",
    "    exp_dict['m3'].append(m3)\n",
    "    print('\\n\\n\\n')\n",
    "    return exp_dict, wfst\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_dict= {\n",
    "#     'loss' : [],\n",
    "#     'efficancy':[],\n",
    "#     'acc': [],\n",
    "#     'm1': [],\n",
    "#     'm2': [],\n",
    "#     'm3': [],\n",
    "#     'all_states': [],\n",
    "#     'all_arcs': [],\n",
    "#     'det': False,\n",
    "#     'type': 'baseline'   \n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# prune_threshold =  1000\n",
    "# det = False\n",
    "# wfst_type = 'tuned'\n",
    "# verbose = False\n",
    "# size = 0.05\n",
    "# print(f'Threshold = {prune_threshold}')\n",
    "# exp_dict, wfst = run_exp(exp_dict, wfst_type, det, verbose=verbose, train_split=size, use_pruning=False, prune_threshold=prune_threshold)\n",
    "# exp_dict['WFST'] = wfst\n",
    "\n",
    "#     # file_name = f'exp_dict_tuned_ours_{prune_threshold}.pkl'\n",
    "#     # with open(file_name, 'wb') as handler:\n",
    "#     #     pickle.dump(exp_dict, handler)\n",
    "#     # print(f'saved to {file_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print the experiment results\n",
    "# print(f'FST type: {exp_dict[\"type\"]}')\n",
    "# print(f'Determinized: {exp_dict[\"det\"]}')\n",
    "# print(f'All states: {exp_dict[\"all_states\"]}, all arcs: {exp_dict[\"all_arcs\"]}')\n",
    "# print(f'WER: {exp_dict[\"acc\"]}')\n",
    "# print(f'M1 (avg. Decoding time): {exp_dict[\"m1\"]}')\n",
    "# print(f'M2 (avg. Backtrace time): {exp_dict[\"m2\"]}')\n",
    "# print(f'M3 (avg. Number of Computations): {exp_dict[\"m3\"]}')\n",
    "\n",
    "# draw(wfst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold = 1000\n",
      "Type tuned, det = False, minimized = False\n",
      "All states: 129, all arcs: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▎                                          | 1/159 [00:03<09:35,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding took 2.7112317085266113 seconds\n",
      "Number of computations: 80040\n",
      "Backtrace took 0.004311561584472656 seconds\n",
      "Transcription: a pickled piper of peter || Prediction: of  || (nsub, ndel, nin) :(0, 4, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                          | 2/159 [00:05<07:31,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding took 1.7237746715545654 seconds\n",
      "Number of computations: 53844\n",
      "Backtrace took 0.0027921199798583984 seconds\n",
      "Transcription: where's peter || Prediction: where's  || (nsub, ndel, nin) :(0, 1, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▊                                          | 3/159 [00:08<07:30,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding took 2.1205577850341797 seconds\n",
      "Number of computations: 65880\n",
      "Backtrace took 0.003423452377319336 seconds\n",
      "Transcription: peter picked a peck || Prediction: peter  || (nsub, ndel, nin) :(0, 3, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█                                          | 4/159 [00:11<07:36,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding took 2.229952573776245 seconds\n",
      "Number of computations: 68004\n",
      "Backtrace took 0.0029468536376953125 seconds\n",
      "Transcription: where's the peppers || Prediction: where's  || (nsub, ndel, nin) :(0, 2, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▎                                         | 5/159 [00:15<07:52,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding took 2.488222599029541 seconds\n",
      "Number of computations: 76028\n",
      "Backtrace took 0.004270076751708984 seconds\n",
      "Transcription: the piper pickled peppers || Prediction: the  || (nsub, ndel, nin) :(0, 3, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▌                                         | 6/159 [00:21<10:28,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding took 4.569878816604614 seconds\n",
      "Number of computations: 140456\n",
      "Backtrace took 0.007609844207763672 seconds\n",
      "Transcription: peter piper picked a peck of pickled peppers || Prediction: a  || (nsub, ndel, nin) :(0, 7, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▉                                         | 7/159 [00:25<10:06,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding took 2.814401149749756 seconds\n",
      "Number of computations: 85940\n",
      "Backtrace took 0.004544496536254883 seconds\n",
      "Transcription: a peck of pickled peppers peter piper picked || Prediction: the  || (nsub, ndel, nin) :(1, 7, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██▏                                        | 8/159 [00:29<10:23,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding took 3.334538698196411 seconds\n",
      "Number of computations: 102224\n",
      "Backtrace took 0.004518985748291016 seconds\n",
      "Transcription: where's the peck of pickled peppers peter piper picked || Prediction: where's  || (nsub, ndel, nin) :(0, 8, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▍                                        | 9/159 [00:33<10:32,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding took 3.275918960571289 seconds\n",
      "Number of computations: 102224\n",
      "Backtrace took 0.004847288131713867 seconds\n",
      "Transcription: peter picked a peck of pickled peppers || Prediction: peter  || (nsub, ndel, nin) :(0, 6, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▋                                       | 10/159 [00:37<10:18,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding took 2.949537515640259 seconds\n",
      "Number of computations: 92076\n",
      "Backtrace took 0.004026651382446289 seconds\n",
      "Transcription: where's the peck of pickled peppers || Prediction: where's  || (nsub, ndel, nin) :(0, 5, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▉                                       | 11/159 [00:41<09:58,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding took 2.841794729232788 seconds\n",
      "Number of computations: 88064\n",
      "Backtrace took 0.004404306411743164 seconds\n",
      "Transcription: pickled peppers peck peter piper || Prediction: pickled  || (nsub, ndel, nin) :(0, 4, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███▏                                      | 12/159 [00:48<11:37,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding took 4.743128538131714 seconds\n",
      "Number of computations: 144468\n",
      "Backtrace took 0.006934642791748047 seconds\n",
      "Transcription: peter piper picked a peck of pickled peppers where's the peck of pickled peppers peter piper picked || Prediction: peter  || (nsub, ndel, nin) :(0, 16, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███▍                                      | 13/159 [00:51<10:21,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding took 2.2785561084747314 seconds\n",
      "Number of computations: 69892\n",
      "Backtrace took 0.003966569900512695 seconds\n",
      "Transcription: peter piper picked peppers || Prediction: the  || (nsub, ndel, nin) :(1, 3, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███▋                                      | 14/159 [00:56<10:48,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding took 3.6811325550079346 seconds\n",
      "Number of computations: 114260\n",
      "Backtrace took 0.005362749099731445 seconds\n",
      "Transcription: peter pickled peppers where's the peppers peter pickled || Prediction: picked  || (nsub, ndel, nin) :(1, 7, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███▉                                      | 15/159 [00:59<09:40,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding took 2.2386813163757324 seconds\n",
      "Number of computations: 69892\n",
      "Backtrace took 0.0032320022583007812 seconds\n",
      "Transcription: peter pickled a peck of peppers || Prediction: picked  || (nsub, ndel, nin) :(1, 5, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████▏                                     | 16/159 [01:02<08:51,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding took 2.170567512512207 seconds\n",
      "Number of computations: 68004\n",
      "Backtrace took 0.003246307373046875 seconds\n",
      "Transcription: peter piper picked a peck of peppers || Prediction: peter  || (nsub, ndel, nin) :(0, 6, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████▍                                     | 17/159 [01:09<11:30,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding took 5.5901288986206055 seconds\n",
      "Number of computations: 172552\n",
      "Backtrace took 0.009855031967163086 seconds\n",
      "Transcription: peter piper picked a peck of pickled peppers where's the peck of pickled peppers peter piper picked || Prediction: peter  || (nsub, ndel, nin) :(0, 16, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████▊                                     | 18/159 [01:12<10:19,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding took 2.437248945236206 seconds\n",
      "Number of computations: 76028\n",
      "Backtrace took 0.0034227371215820312 seconds\n",
      "Transcription: where's the peppers peter picked || Prediction: where's  || (nsub, ndel, nin) :(0, 4, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████                                     | 19/159 [01:16<09:56,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding took 2.9325191974639893 seconds\n",
      "Number of computations: 91604\n",
      "Backtrace took 0.00400996208190918 seconds\n",
      "Transcription: a peck of peter picked || Prediction: of  || (nsub, ndel, nin) :(0, 4, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████▎                                    | 20/159 [01:22<10:44,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding took 4.100403308868408 seconds\n",
      "Number of computations: 126768\n",
      "Backtrace took 0.005964040756225586 seconds\n",
      "Transcription: peter piper picked a peck of pickled peppers || Prediction: peter  || (nsub, ndel, nin) :(0, 7, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████▌                                    | 21/159 [01:28<11:55,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding took 4.815954923629761 seconds\n",
      "Number of computations: 148716\n",
      "Backtrace took 0.007529497146606445 seconds\n",
      "Transcription: where's peppers peter a peck of pickled peter || Prediction: where's  || (nsub, ndel, nin) :(0, 7, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████▊                                    | 22/159 [01:31<10:13,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding took 2.093461036682129 seconds\n",
      "Number of computations: 65408\n",
      "Backtrace took 0.002949953079223633 seconds\n",
      "Transcription: peppers picked peter || Prediction: peppers  || (nsub, ndel, nin) :(0, 2, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|██████                                    | 23/159 [01:42<14:11,  6.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding took 7.543212652206421 seconds\n",
      "Number of computations: 234148\n",
      "Backtrace took 0.011068344116210938 seconds\n",
      "Transcription: peter piper picked a peck of pickled peppers where's the peck of pickled peppers peter piper picked || Prediction: peter  || (nsub, ndel, nin) :(0, 16, 0)\n"
     ]
    }
   ],
   "source": [
    "exp_dict= {\n",
    "    'loss' : [],\n",
    "    'efficancy':[],\n",
    "    'acc': [],\n",
    "    'm1': [],\n",
    "    'm2': [],\n",
    "    'm3': [],\n",
    "    'all_states': [],\n",
    "    'all_arcs': [],\n",
    "    'det': False,\n",
    "    'type': 'baseline'   \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "prune_threshold =  1000\n",
    "det = False\n",
    "wfst_type = 'tuned'\n",
    "verbose = True\n",
    "size = 0.5\n",
    "print(f'Threshold = {prune_threshold}')\n",
    "exp_dict, wfst = run_exp(exp_dict, wfst_type, det, verbose=verbose, train_split=size, use_pruning=False, prune_threshold=prune_threshold)\n",
    "exp_dict['WFST'] = wfst\n",
    "\n",
    "    # file_name = f'exp_dict_tuned_ours_{prune_threshold}.pkl'\n",
    "    # with open(file_name, 'wb') as handler:\n",
    "    #     pickle.dump(exp_dict, handler)\n",
    "    # print(f'saved to {file_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FST type: tuned\n",
      "Determinized: False\n",
      "All states: 129, all arcs: 256\n",
      "WER: [0.8527388938598706]\n",
      "M1 (avg. Decoding time): [3.52302189742994]\n",
      "M2 (avg. Backtrace time): [0.005438210829248968]\n",
      "M3 (avg. Number of Computations): [108457.96226415095]\n"
     ]
    }
   ],
   "source": [
    "# print the experiment results\n",
    "print(f'FST type: {exp_dict[\"type\"]}')\n",
    "print(f'Determinized: {exp_dict[\"det\"]}')\n",
    "print(f'All states: {exp_dict[\"all_states\"]}, all arcs: {exp_dict[\"all_arcs\"]}')\n",
    "print(f'WER: {exp_dict[\"acc\"]}')\n",
    "print(f'M1 (avg. Decoding time): {exp_dict[\"m1\"]}')\n",
    "print(f'M2 (avg. Backtrace time): {exp_dict[\"m2\"]}')\n",
    "print(f'M3 (avg. Number of Computations): {exp_dict[\"m3\"]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dict= {\n",
    "    'loss' : [],\n",
    "    'efficancy':[],\n",
    "    'acc': [],\n",
    "    'm1': [],\n",
    "    'm2': [],\n",
    "    'm3': [],\n",
    "    'all_states': [],\n",
    "    'all_arcs': [],\n",
    "    'det': False,\n",
    "    'type': 'baseline'   \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "prune_threshold =  1000\n",
    "det = False\n",
    "wfst_type = 'tuned_unigram'\n",
    "verbose = False\n",
    "size = 0.01\n",
    "print(f'Threshold = {prune_threshold}')\n",
    "exp_dict, wfst = run_exp(exp_dict, wfst_type, det, verbose=verbose, train_split=size, use_pruning=False, prune_threshold=prune_threshold)\n",
    "exp_dict['WFST'] = wfst\n",
    "\n",
    "    # file_name = f'exp_dict_tuned_ours_{prune_threshold}.pkl'\n",
    "    # with open(file_name, 'wb') as handler:\n",
    "    #     pickle.dump(exp_dict, handler)\n",
    "    # print(f'saved to {file_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the experiment results\n",
    "print(f'FST type: {exp_dict[\"type\"]}')\n",
    "print(f'Determinized: {exp_dict[\"det\"]}')\n",
    "print(f'All states: {exp_dict[\"all_states\"]}, all arcs: {exp_dict[\"all_arcs\"]}')\n",
    "print(f'WER: {exp_dict[\"acc\"]}')\n",
    "print(f'M1 (avg. Decoding time): {exp_dict[\"m1\"]}')\n",
    "print(f'M2 (avg. Backtrace time): {exp_dict[\"m2\"]}')\n",
    "print(f'M3 (avg. Number of Computations): {exp_dict[\"m3\"]}')\n",
    "\n",
    "draw(wfst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use figure size (width, height) in inches\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# create a plot with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "# plot the first subplot\n",
    "ax1.plot(prune_thresholds, exp_dict['acc'], 'o-')\n",
    "ax1.set_title('WER')\n",
    "ax1.set_xlabel('Prune Threshold')\n",
    "ax1.set_ylabel('WER')\n",
    "\n",
    "# plot the second subplot\n",
    "ax2.plot(prune_thresholds, exp_dict['m1'], 'o-')\n",
    "ax2.set_title('Decoding Time')\n",
    "ax2.set_xlabel('Prune Threshold')\n",
    "ax2.set_ylabel('Decoding Time')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "file_name_baseline_ours = 'exp_dict_baseline_ours_100.pkl'\n",
    "file_name_baseline_histogram = 'exp_dict_baseline_histogram_200.pkl'\n",
    "file_name_tuned_ours = 'exp_dict_tuned_ours_100.pkl'\n",
    "file_name_tuned_histogram = 'exp_dict_tuned_histogram_200.pkl'\n",
    "\n",
    "with open(file_name_baseline_ours, 'rb') as handler:\n",
    "    exp_dict_baseline_ours = pickle.load(handler)\n",
    "\n",
    "with open(file_name_baseline_histogram, 'rb') as handler:\n",
    "    exp_dict_baseline_histogram = pickle.load(handler)\n",
    "\n",
    "with open(file_name_tuned_ours, 'rb') as handler:\n",
    "    exp_dict_tuned_ours = pickle.load(handler)\n",
    "\n",
    "with open(file_name_tuned_histogram, 'rb') as handler:\n",
    "    exp_dict_tuned_histogram = pickle.load(handler)\n",
    "\n",
    "current_exp_dict1 = exp_dict_baseline_ours\n",
    "prune_thresholdso = [i*5 for i in range(1,21)]\n",
    "\n",
    "current_exp_dict2 = exp_dict_baseline_histogram\n",
    "prune_thresholdse = [i*10 for i in range(1,21)]\n",
    "\n",
    "current_exp_dict3 = exp_dict_tuned_ours\n",
    "prune_thresholdso = [i*5 for i in range(1,21)]\n",
    "\n",
    "current_exp_dict4 = exp_dict_tuned_histogram\n",
    "prune_thresholdse = [i*10 for i in range(1,21)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# plot the accuracy and decoding time vs prune threshold in the same plot with two y axes\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Prune Threshold')\n",
    "ax1.set_ylabel('WER', color=color)\n",
    "ax1.plot(prune_thresholdso, current_exp_dict1['acc'], 'o-', color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Decoding Time', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(prune_thresholdso, current_exp_dict1['m1'], 'o-', color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# plot the accuracy and decoding time vs prune threshold in the same plot with two y axes\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Prune Threshold')\n",
    "ax1.set_ylabel('WER', color=color)\n",
    "ax1.plot(prune_thresholdse, current_exp_dict2['acc'], 'o-', color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Decoding Time', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(prune_thresholdse, current_exp_dict2['m1'], 'o-', color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# plot the accuracy and decoding time vs prune threshold in the same plot with two y axes\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Prune Threshold')\n",
    "ax1.set_ylabel('WER', color=color)\n",
    "ax1.plot(prune_thresholdso, current_exp_dict3['acc'], 'o-', color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Decoding Time', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(prune_thresholdso, current_exp_dict3['m1'], 'o-', color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.savefig('prune.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# plot the accuracy and decoding time vs prune threshold in the same plot with two y axes\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Prune Threshold')\n",
    "ax1.set_ylabel('WER', color=color)\n",
    "ax1.plot(prune_thresholdse, current_exp_dict4['acc'], 'o-', color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Decoding Time', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(prune_thresholdse, current_exp_dict4['m1'], 'o-', color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the best prune threshold\n",
    "current_exp_dict = current_exp_dict1\n",
    "best_prune_threshold = prune_thresholdso[np.argmin(current_exp_dict['acc'])]\n",
    "print(f'Best prune threshold: {best_prune_threshold}')\n",
    "print(f'WER: {current_exp_dict[\"acc\"][np.argmin(current_exp_dict[\"acc\"])]}')\n",
    "print(f'Decoding Time: {current_exp_dict[\"m1\"][np.argmin(current_exp_dict[\"acc\"])]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the best prune threshold\n",
    "current_exp_dict = current_exp_dict2\n",
    "best_prune_threshold = prune_thresholdse[np.argmin(current_exp_dict['acc'])]\n",
    "print(f'Best prune threshold: {best_prune_threshold}')\n",
    "print(f'WER: {current_exp_dict[\"acc\"][np.argmin(current_exp_dict[\"acc\"])]}')\n",
    "print(f'Decoding Time: {current_exp_dict[\"m1\"][np.argmin(current_exp_dict[\"acc\"])]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the best prune threshold\n",
    "current_exp_dict = current_exp_dict3\n",
    "best_prune_threshold = prune_thresholdso[np.argmin(current_exp_dict['acc'])]\n",
    "print(f'Best prune threshold: {best_prune_threshold}')\n",
    "print(f'WER: {current_exp_dict[\"acc\"][np.argmin(current_exp_dict[\"acc\"])]}')\n",
    "print(f'Decoding Time: {current_exp_dict[\"m1\"][np.argmin(current_exp_dict[\"acc\"])]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best prune threshold\n",
    "current_exp_dict = current_exp_dict4\n",
    "best_prune_threshold = prune_thresholdse[np.argmin(current_exp_dict['acc'])]\n",
    "print(f'Best prune threshold: {best_prune_threshold}')\n",
    "print(f'WER: {current_exp_dict[\"acc\"][np.argmin(current_exp_dict[\"acc\"])]}')\n",
    "print(f'Decoding Time: {current_exp_dict[\"m1\"][np.argmin(current_exp_dict[\"acc\"])]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "8696358051874a88c1d2911df8bbed88795a658bdc69c38e8bd7d9488224210f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
