{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wer\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import observation_model\n",
    "import openfst_python as fst\n",
    "import matplotlib.pyplot as plt\n",
    "from subprocess import check_call\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "#===============================================================================\n",
    "# ============================== UTILITY FUNCTIONS =============================\n",
    "#===============================================================================\n",
    "\n",
    "def read_transcription(wav_file):\n",
    "    \"\"\"\n",
    "    Get the transcription corresponding to wav_file.\n",
    "    \"\"\"\n",
    "    \n",
    "    transcription_file = os.path.splitext(wav_file)[0] + '.txt'\n",
    "    \n",
    "    with open(transcription_file, 'r') as f:\n",
    "        transcription = f.readline().strip()\n",
    "    \n",
    "    return transcription\n",
    "\n",
    "\n",
    "def get_unigram_and_bigram_probs():\n",
    "    i = 0\n",
    "    train_split = int(0.5 * len(glob.glob('/group/teaching/asr/labs/recordings/*.wav')))  \n",
    "\n",
    "\n",
    "    all_transcriptions = ''\n",
    "    for wav_file in glob.glob('/group/teaching/asr/labs/recordings/*.wav'):   \n",
    "\n",
    "        transcription = read_transcription(wav_file)\n",
    "        all_transcriptions += transcription + ' \\n'\n",
    "\n",
    "        i += 1\n",
    "        if i == train_split:\n",
    "            break\n",
    "\n",
    "    # count unigram counts in all_transcriptions\n",
    "    unigram_counts = {}\n",
    "    for word in all_transcriptions.replace(\"\\n\", \" \").split():\n",
    "        if word in unigram_counts:\n",
    "            unigram_counts[word] += 1\n",
    "        else:\n",
    "            unigram_counts[word] = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    unigram_probs = {}\n",
    "    for word, count in unigram_counts.items():\n",
    "        unigram_probs[word] = count / sum(unigram_counts.values())\n",
    "\n",
    "\n",
    "    # save unigram probs to pickle file\n",
    "    import pickle\n",
    "    with open('unigram_probs.pickle', 'wb') as handle:\n",
    "        pickle.dump(unigram_probs, handle)\n",
    "        # load unigram probs from pickle file\n",
    "    import pickle\n",
    "    with open('unigram_probs.pickle', 'rb') as handle:\n",
    "        unigram_probs = pickle.load(handle)\n",
    "\n",
    "    # count unigram counts in all_transcriptions\n",
    "    bigram_counts = {}\n",
    "    for line in all_transcriptions.split(\"\\n\"):\n",
    "        line = line.split()\n",
    "        for idx, word in enumerate(line):\n",
    "            if idx > 0 and (line[idx - 1], word) in bigram_counts:\n",
    "                bigram_counts[(line[idx - 1], word)] += 1\n",
    "            elif idx == 0 and (\"<start>\", word) in bigram_counts:\n",
    "                bigram_counts[(\"<start>\", word)] += 1\n",
    "            elif idx == 0 and (\"<start>\", word) not in bigram_counts:\n",
    "                bigram_counts[(\"<start>\", word)] = 1\n",
    "            else:\n",
    "                bigram_counts[(line[idx - 1], word)] = 1\n",
    "\n",
    "    # calculate bigram probabilities\n",
    "    bigram_probs = {}\n",
    "    for bigram, count in bigram_counts.items():\n",
    "        if bigram[0] == \"<start>\":\n",
    "            bigram_probs[bigram] = count / len(all_transcriptions.split(\"\\n\"))\n",
    "        else:\n",
    "            bigram_probs[bigram] = count / unigram_counts[bigram[0]]\n",
    "\n",
    "    return unigram_probs, bigram_probs\n",
    "\n",
    "\n",
    "def draw(f):\n",
    "    f.draw('tmp.dot', portrait=True)\n",
    "    check_call(['dot','-Tpng','-Gdpi=1800','tmp.dot','-o','tmp.png'])\n",
    "    return Image(filename='tmp.png')\n",
    "\n",
    "\n",
    "def parse_lexicon(lex_file):\n",
    "    \"\"\"\n",
    "    Parse the lexicon file and return it in dictionary form.\n",
    "    \n",
    "    Args:\n",
    "        lex_file (str): filename of lexicon file with structure '<word> <phone1> <phone2>...'\n",
    "                        eg. peppers p eh p er z\n",
    "\n",
    "    Returns:\n",
    "        lex (dict): dictionary mapping words to list of phones\n",
    "    \"\"\"\n",
    "    \n",
    "    lex = {}  # create a dictionary for the lexicon entries (this could be a problem with larger lexica)\n",
    "    with open(lex_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.split()  # split at each space\n",
    "            lex[line[0]] = line[1:]  # first field the word, the rest is the phones\n",
    "    return lex\n",
    "\n",
    "\n",
    "def generate_symbol_tables(lexicon, n=3, with_silence=True):\n",
    "    '''\n",
    "    Return word, phone and state symbol tables based on the supplied lexicon\n",
    "        \n",
    "    Args:\n",
    "        lexicon (dict): lexicon to use, created from the parse_lexicon() function\n",
    "        n (int): number of states for each phone HMM\n",
    "        \n",
    "    Returns:\n",
    "        word_table (fst.SymbolTable): table of words\n",
    "        phone_table (fst.SymbolTable): table of phones\n",
    "        state_table (fst.SymbolTable): table of HMM phone-state IDs\n",
    "    '''\n",
    "    \n",
    "    state_table = fst.SymbolTable()\n",
    "    phone_table = fst.SymbolTable()\n",
    "    word_table = fst.SymbolTable()\n",
    "    \n",
    "    # add empty <eps> symbol to all tables\n",
    "    state_table.add_symbol('<eps>')\n",
    "    phone_table.add_symbol('<eps>')\n",
    "    word_table.add_symbol('<eps>')\n",
    "    \n",
    "    for word, phones  in lexicon.items():\n",
    "        \n",
    "        word_table.add_symbol(word)\n",
    "        \n",
    "        for p in phones: # for each phone\n",
    "            \n",
    "            phone_table.add_symbol(p)\n",
    "            for i in range(1,n+1): # for each state 1 to n\n",
    "                state_table.add_symbol('{}_{}'.format(p, i))\n",
    "\n",
    "    if with_silence:\n",
    "        # add silence symbols\n",
    "        phone_table.add_symbol('sil')\n",
    "        for i in range(1,n+1):\n",
    "            state_table.add_symbol('sil_{}'.format(i))\n",
    "            \n",
    "    return word_table, phone_table, state_table\n",
    "\n",
    "#===============================================================================\n",
    "# ============================== VITERBI DECODING ==============================\n",
    "#===============================================================================\n",
    "\n",
    "class MyViterbiDecoder:\n",
    "    \n",
    "    NLL_ZERO = 1e10 \n",
    "    \n",
    "    def __init__(self, f, audio_file_name, verbose=False, use_pruning=False, determinized=False, bigram = False, histogram_pruning_threshold = 0):\n",
    "        \"\"\"Set up the decoder class with an audio file and WFST f\n",
    "        \"\"\"\n",
    "        self.lex = parse_lexicon('lexicon.txt')\n",
    "        self.verbose = verbose\n",
    "        self.om = observation_model.ObservationModel()\n",
    "        self.f = f\n",
    "        self.number_of_computiations = 0\n",
    "        self.decode_time = 0\n",
    "        self.backtrace_time = 0\n",
    "        self.use_pruning = use_pruning\n",
    "        self.prune_threshold = self.NLL_ZERO\n",
    "        self.determinized = determinized\n",
    "        self.bigram = bigram\n",
    "        self.word_start = -1\n",
    "        self.histogram_pruning_threshold = histogram_pruning_threshold\n",
    "\n",
    "\n",
    "        for state in self.f.states():\n",
    "            for arc in self.f.arcs(state):\n",
    "                o = self.f.output_symbols().find(arc.olabel)\n",
    "                if o =='sil':\n",
    "                    self.word_start = arc.nextstate\n",
    "\n",
    "        if self.word_start == -1:\n",
    "            if self.determinized:\n",
    "                self.word_start = 1\n",
    "                   \n",
    "                    \n",
    "\n",
    "        if self.use_pruning:\n",
    "            self.prune_threshold = 30.0\n",
    "        \n",
    "        if audio_file_name:\n",
    "            self.om.load_audio(audio_file_name)\n",
    "        else:\n",
    "            self.om.load_dummy_audio()\n",
    "        \n",
    "        self.initialise_decoding()\n",
    "\n",
    "        \n",
    "    def initialise_decoding(self):\n",
    "        \"\"\"set up the values for V_j(0) (as negative log-likelihoods)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.V = []   # stores likelihood along best path reaching state j\n",
    "        self.B = []   # stores identity of best previous state reaching state j\n",
    "        self.W = []   # stores output labels sequence along arc reaching j - this removes need for \n",
    "                      # extra code to read the output sequence along the best path\n",
    "        \n",
    "        for t in range(self.om.observation_length()+1):\n",
    "            self.V.append([self.NLL_ZERO]*self.f.num_states())\n",
    "            self.B.append([-1]*self.f.num_states())\n",
    "            self.W.append([[] for i in range(self.f.num_states())])  #  multiplying the empty list doesn't make multiple\n",
    "        \n",
    "        # The above code means that self.V[t][j] for t = 0, ... T gives the Viterbi cost\n",
    "        # of state j, time t (in negative log-likelihood form)\n",
    "        # Initialising the costs to NLL_ZERO effectively means zero probability    \n",
    "        \n",
    "        # give the WFST start state a probability of 1.0   (NLL = 0.0)\n",
    "        self.V[0][self.f.start()] = 0.0\n",
    "        \n",
    "        # some WFSTs might have arcs with epsilon on the input (you might have already created \n",
    "        # examples of these in earlier labs) these correspond to non-emitting states, \n",
    "        # which means that we need to process them without stepping forward in time.  \n",
    "        # Don't worry too much about this!  \n",
    "        self.traverse_epsilon_arcs(0)        \n",
    "\n",
    "\n",
    "    def traverse_epsilon_arcs(self, t):\n",
    "        \"\"\"Traverse arcs with <eps> on the input at time t\n",
    "        \n",
    "        These correspond to transitions that don't emit an observation\n",
    "        \n",
    "        We've implemented this function for you as it's slightly trickier than\n",
    "        the normal case.  You might like to look at it to see what's going on, but\n",
    "        don't worry if you can't fully follow it.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        states_to_traverse = list(self.f.states()) # traverse all states\n",
    "        while states_to_traverse:\n",
    "            \n",
    "            # Set i to the ID of the current state, the first \n",
    "            # item in the list (and remove it from the list)\n",
    "            i = states_to_traverse.pop(0)   \n",
    "        \n",
    "            # don't bother traversing states which have zero probability\n",
    "            if self.V[t][i] == self.NLL_ZERO:\n",
    "                    continue\n",
    "        \n",
    "            for arc in self.f.arcs(i):\n",
    "                \n",
    "                if arc.ilabel == 0:     # if <eps> transition\n",
    "                  \n",
    "                    j = arc.nextstate   # ID of next state  \n",
    "                \n",
    "                    if self.V[t][j] > self.V[t][i] + float(arc.weight):\n",
    "                        \n",
    "                        # this means we've found a lower-cost path to\n",
    "                        # state j at time t.  We might need to add it\n",
    "                        # back to the processing queue.\n",
    "                        self.V[t][j] = self.V[t][i] + float(arc.weight)\n",
    "                        \n",
    "                        # save backtrace information.  In the case of an epsilon transition, \n",
    "                        # we save the identity of the best state at t-1.  This means we may not\n",
    "                        # be able to fully recover the best path, but to do otherwise would\n",
    "                        # require a more complicated way of storing backtrace information\n",
    "                        self.B[t][j] = self.B[t][i] \n",
    "                        \n",
    "                        # and save the output labels encountered - this is a list, because\n",
    "                        # there could be multiple output labels (in the case of <eps> arcs)\n",
    "                        if arc.olabel != 0:\n",
    "                            self.W[t][j] = self.W[t][i] + [arc.olabel]\n",
    "                        else:\n",
    "                            self.W[t][j] = self.W[t][i]\n",
    "                        \n",
    "                        if j not in states_to_traverse:\n",
    "                            states_to_traverse.append(j)\n",
    "\n",
    "\n",
    "    def forward_step(self, t):\n",
    "        '''\n",
    "        Propagate the Viterbi costs forward one time step\n",
    "        '''\n",
    "        for i in self.f.states():\n",
    "            \n",
    "            if not self.V[t-1][i] == self.NLL_ZERO:   # no point in propagating states with zero probability\n",
    "                \n",
    "                for arc in self.f.arcs(i):\n",
    "                    \n",
    "                    \n",
    "                    if arc.ilabel != 0: # <eps> transitions don't emit an observation\n",
    "                        self.number_of_computiations += 1\n",
    "                        j = arc.nextstate\n",
    "                        tp = float(arc.weight)  # transition prob\n",
    "                        ep = -self.om.log_observation_probability(self.f.input_symbols().find(arc.ilabel), t)  # emission negative log prob\n",
    "\n",
    "\n",
    "                        prob = tp + ep + self.V[t-1][i] # they're logs\n",
    "\n",
    "                        \n",
    "\n",
    "                        if prob < self.V[t][j]:\n",
    "                            self.V[t][j] = prob\n",
    "                            self.B[t][j] = i\n",
    "                            \n",
    "                            # store the output labels encountered too\n",
    "                            if arc.olabel !=0:\n",
    "                                self.W[t][j] = [arc.olabel]\n",
    "                            else:\n",
    "                                self.W[t][j] = []\n",
    "\n",
    "        if self.use_pruning and self.histogram_pruning_threshold == 0:    \n",
    "            best_path = min(self.V[t])\n",
    "            for idx, path in enumerate(self.V[t]):\n",
    "                if path - best_path > self.prune_threshold:\n",
    "                    self.V[t][idx] = self.NLL_ZERO\n",
    "\n",
    "        if self.use_pruning and self.histogram_pruning_threshold > 0:\n",
    "            # get the indices and values of paths in V[t] that do not have NLL_ZERO probability\n",
    "            indices = [i for i, x in enumerate(self.V[t]) if x != self.NLL_ZERO]\n",
    "            values = [x for x in self.V[t] if x != self.NLL_ZERO]\n",
    "\n",
    "            # check if there are more than histogram_threshold paths\n",
    "            if len(values) > self.histogram_pruning_threshold:\n",
    "                # keep the histogram_threshold best paths and set the rest to NLL_ZERO\n",
    "                best_paths = np.argpartition(values, self.histogram_pruning_threshold)[:self.histogram_pruning_threshold]\n",
    "                for idx in indices:\n",
    "                    if idx not in best_paths:\n",
    "                        self.V[t][idx] = self.NLL_ZERO\n",
    "            \n",
    "                        \n",
    "    def finalise_decoding(self):\n",
    "        \"\"\" this incorporates the probability of terminating at each state\n",
    "        \"\"\"\n",
    "        \n",
    "        for state in self.f.states():\n",
    "            final_weight = float(self.f.final(state))\n",
    "            if self.V[-1][state] != self.NLL_ZERO:\n",
    "                if final_weight == math.inf:\n",
    "                    self.V[-1][state] = self.NLL_ZERO  # effectively says that we can't end in this state\n",
    "                else:\n",
    "                    self.V[-1][state] += final_weight\n",
    "                    \n",
    "        # get a list of all states where there was a path ending with non-zero probability\n",
    "        finished = [x for x in self.V[-1] if x < self.NLL_ZERO]\n",
    "        if not finished and self.verbose:  # if empty\n",
    "            print(\"No path got to the end of the observations.\")\n",
    "        \n",
    "        \n",
    "    def decode(self):\n",
    "        start = time.time()\n",
    "        self.initialise_decoding()\n",
    "        t = 1\n",
    "        while t <= self.om.observation_length():\n",
    "            self.forward_step(t)\n",
    "            self.traverse_epsilon_arcs(t)\n",
    "            t += 1\n",
    "            \n",
    "        self.finalise_decoding()\n",
    "        end = time.time()\n",
    "        self.decode_time = end-start\n",
    "        if self.verbose:\n",
    "            print(\"Decoding took\", self.decode_time,  \"seconds\")\n",
    "            print(\"Number of computations:\", self.number_of_computiations)\n",
    "\n",
    "    \n",
    "    def backtrace(self):\n",
    "        start = time.time()\n",
    "        best_final_state = self.V[-1].index(min(self.V[-1])) # argmin\n",
    "        best_state_sequence = [best_final_state]\n",
    "        best_out_sequence = []\n",
    "        \n",
    "        t = self.om.observation_length()   # ie T\n",
    "        j = best_final_state\n",
    "\n",
    "\n",
    "        next_i = -1\n",
    "        prev_i = -1\n",
    "\n",
    " \n",
    "        while t >= 0:\n",
    "            i = self.B[t][j]\n",
    "            best_state_sequence.append(i)\n",
    "\n",
    "            if self.determinized:\n",
    "            \n",
    "                if i == self.word_start and self.determinized:\n",
    "                    best_out_sequence = self.W[t][j] + [0] + best_out_sequence\n",
    "                else: \n",
    "                    best_out_sequence = self.W[t][j] + best_out_sequence                              \n",
    "\n",
    "\n",
    "            else:\n",
    "                is_final =  False\n",
    "                \n",
    "                if i >=0:\n",
    "                    for arc in self.f.arcs(i):\n",
    "                        next_i = arc.nextstate\n",
    "                        if next_i != i and i != prev_i:\n",
    "                            if float(self.f.final(next_i)) != math.inf:\n",
    "                                best_out_sequence = self.W[t][j] + [0] + best_out_sequence\n",
    "                                is_final = True\n",
    "                    if not is_final:\n",
    "                        best_out_sequence = self.W[t][j] + best_out_sequence\n",
    "\n",
    "                    prev_i = i\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            # continue the backtrace at state i, time t-1\n",
    "            j = i  \n",
    "            t-=1\n",
    "            \n",
    "        best_state_sequence.reverse()\n",
    "        \n",
    "        \n",
    "        best_out_sequence = ' '.join([ self.f.output_symbols().find(label) for label in best_out_sequence])\n",
    "\n",
    "        if self.bigram:\n",
    "            split_word = \"sil\"\n",
    "        else:\n",
    "            split_word = \"<eps>\"\n",
    "\n",
    "        phones = best_out_sequence.split(split_word)\n",
    "        phones[0] = phones[0].replace(\"sil\",\"\")\n",
    "        # print('Phones: ', phones)\n",
    "        # print('best_out_sequence: ', best_out_sequence)\n",
    "        # print('best_state_sequence: ', best_state_sequence)\n",
    "        words = \"\"\n",
    "\n",
    "        for phone in phones:\n",
    "            phone = phone.replace(' ', '')\n",
    "            for key, val in self.lex.items():\n",
    "                val = ''.join(val)\n",
    "                if val == phone:\n",
    "                    words += key + \" \"\n",
    "\n",
    "\n",
    "        end = time.time()\n",
    "        self.backtrace_time = end-start\n",
    "        if self.verbose:\n",
    "            print(\"Backtrace took\", self.backtrace_time, \"seconds\")\n",
    "        \n",
    "        return (best_state_sequence, words)\n",
    "\n",
    "\n",
    "#===================================================================================================\n",
    "#======================================  HMM  ======================================================\n",
    "#===================================================================================================\n",
    "\n",
    "\n",
    "unigram_probs, bigram_probs = get_unigram_and_bigram_probs()\n",
    "lex = parse_lexicon('lexicon.txt')\n",
    "word_table, phone_table, state_table = generate_symbol_tables(lex)\n",
    "\n",
    "\n",
    "def generate_phone_wfst(f, start_state, phone, n, sl_prop=0.1, tl_prop=0.9):\n",
    "    \"\"\"\n",
    "    Generate a WFST representating an n-state left-to-right phone HMM\n",
    "    \n",
    "    Args:\n",
    "        f (fst.Fst()): an FST object, assumed to exist already\n",
    "        start_state (int): the index of the first state, assmed to exist already\n",
    "        phone (str): the phone label \n",
    "        n (int): number of states for each phone HMM\n",
    "        \n",
    "    Returns:\n",
    "        the final state of the FST\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    current_state = start_state\n",
    "    \n",
    "    for i in range(1, n+1):\n",
    "        \n",
    "        in_label = state_table.find('{}_{}'.format(phone, i))\n",
    "        \n",
    "        sl_weight = fst.Weight('tropical', -math.log(sl_prop))  # weight for self-loop\n",
    "        # self-loop back to current state\n",
    "        f.add_arc(current_state, fst.Arc(in_label, 0, sl_weight, current_state))\n",
    "        \n",
    "        # transition to next state\n",
    "        \n",
    "        # we want to output the phone label on the final state\n",
    "        # note: if outputting words instead this code should be modified\n",
    "        if i == n:\n",
    "            out_label = phone_table.find(phone)\n",
    "        else:\n",
    "            out_label = 0   # output empty <eps> label\n",
    "            \n",
    "        next_state = f.add_state()\n",
    "        next_weight = fst.Weight('tropical', -math.log(tl_prop)) # weight to next state\n",
    "        f.add_arc(current_state, fst.Arc(in_label, out_label, next_weight, next_state))    \n",
    "       \n",
    "        cp = current_state\n",
    "        current_state = next_state\n",
    "\n",
    "    return current_state\n",
    "\n",
    "\n",
    "def generate_word_sequence_recognition_wfst(n, sl_prop=0.1, tl_prop=0.9):\n",
    "    \"\"\" generate a HMM to recognise any single word sequence for words in the lexicon\n",
    "    \n",
    "    Args:\n",
    "        n (int): states per phone HMM\n",
    "\n",
    "    Returns:\n",
    "        the constructed WFST\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    f = fst.Fst()\n",
    "    \n",
    "    # create a single start state\n",
    "    start_state = f.add_state()\n",
    "    f.set_start(start_state)\n",
    "    \n",
    "    for word, phones in lex.items():\n",
    "        current_state = f.add_state()\n",
    "        f.add_arc(start_state, fst.Arc(0, 0, None, current_state))\n",
    "        \n",
    "        for phone in phones: \n",
    "            current_state = generate_phone_wfst(f, current_state, phone, n, sl_prop, tl_prop)\n",
    "        # note: new current_state is now set to the final state of the previous phone WFST\n",
    "        \n",
    "        f.set_final(current_state)\n",
    "        f.add_arc(current_state, fst.Arc(0, 0, None, start_state))\n",
    "        \n",
    "    return f\n",
    "\n",
    "\n",
    "def generate_word_sequence_recognition_wfst_with_silence(n, use_unigram_probs=False, sl_prop=0.1, tl_prop=0.9):\n",
    "    \"\"\" generate a HMM to recognise any single word sequence for words in the lexicon with 2 silence states at the beginning and end\n",
    "    \n",
    "    Args:\n",
    "        n (int): states per phone HMM\n",
    "\n",
    "    Returns:\n",
    "        the constructed WFST\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    f = fst.Fst()\n",
    "\n",
    "    # create a single start state\n",
    "    start_state = f.add_state()\n",
    "    f.set_start(start_state)\n",
    "\n",
    "    silence_state = generate_phone_wfst(f, start_state, 'sil', 3, sl_prop, tl_prop)\n",
    "    i = 0\n",
    "    for word, phones in lex.items():\n",
    "        \n",
    "        \n",
    "        current_state = f.add_state()\n",
    "        if use_unigram_probs:\n",
    "            unigram_weight = fst.Weight('tropical', -math.log(unigram_probs[word]))\n",
    "        else:\n",
    "            unigram_weight = None\n",
    "        f.add_arc(silence_state, fst.Arc(0, 0, unigram_weight , current_state))\n",
    "\n",
    "        for index, phone in enumerate(phones):\n",
    "\n",
    "            current_state = generate_phone_wfst(f, current_state, phone, n, sl_prop, tl_prop)\n",
    "        # note: new current_state is now set to the final state of the previous phone WFST\n",
    "\n",
    "        f.set_final(current_state)\n",
    "        f.add_arc(current_state, fst.Arc(0, 0, None, silence_state))\n",
    "\n",
    "\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def generate_bigram_wfst(n):\n",
    "    \"\"\" generate a HMM to recognise any single word sequence for words in the lexicon with 2 silence states at the beginning and end\n",
    "    \n",
    "    Args:\n",
    "        n (int): states per phone HMM\n",
    "\n",
    "    Returns:\n",
    "        the constructed WFST\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    f = fst.Fst()\n",
    "\n",
    "    # create a single start state\n",
    "    start_state = f.add_state()\n",
    "    f.set_start(start_state)\n",
    "    silence_state = generate_phone_wfst(f, start_state, 'sil', 3)\n",
    "\n",
    "\n",
    "\n",
    "    word_states_dict = {}\n",
    "    for word, phones in lex.items():\n",
    "        word_state = f.add_state()\n",
    "        bigram_start_weight = fst.Weight('tropical', 1e10)\n",
    "        if ('<start>', word) in bigram_probs:\n",
    "            bigram_start_weight = fst.Weight('tropical', -math.log(bigram_probs[('<start>',word)]))\n",
    "\n",
    "        f.add_arc(silence_state, fst.Arc(0, 0, bigram_start_weight, word_state))\n",
    "        word_states_dict[word] = word_state\n",
    "        f.set_final(word_state)\n",
    "        \n",
    "    for key, value in word_states_dict.items():\n",
    "        for word, phones in lex.items():\n",
    "            current_state = f.add_state()\n",
    "\n",
    "\n",
    "            bigram_weight = fst.Weight('tropical', 1e10)\n",
    "            \n",
    "            if (key, word)  in bigram_probs:\n",
    "                bigram_weight = fst.Weight('tropical', -math.log(bigram_probs[(key,word)]))\n",
    "\n",
    "            f.add_arc(value, fst.Arc(0, 0, bigram_weight, current_state))\n",
    "            for index, phone in enumerate(phones):\n",
    "\n",
    "                current_state = generate_phone_wfst(f, current_state, phone, n)\n",
    "            \n",
    "            silence_state = generate_phone_wfst(f, current_state, 'sil', 3)\n",
    "\n",
    "            f.add_arc(silence_state, fst.Arc(0, 0, None, word_states_dict[word]))\n",
    "    return f\n",
    "\n",
    "\n",
    "#==============================================================================\n",
    "#========================== Set up the Experiment =============================\n",
    "#==============================================================================\n",
    "\n",
    "def create_wfst(n):\n",
    "    f = generate_word_sequence_recognition_wfst(n)\n",
    "    f.set_input_symbols(state_table)\n",
    "    f.set_output_symbols(phone_table)\n",
    "    return f\n",
    "\n",
    "\n",
    "def create_tuned_wfst(n):\n",
    "    f = generate_word_sequence_recognition_wfst_with_silence(n, sl_prop=0.95, tl_prop=0.05)\n",
    "    f.set_input_symbols(state_table)\n",
    "    f.set_output_symbols(phone_table)\n",
    "    return f\n",
    "\n",
    "\n",
    "def create_wfst_with_silence(n, use_unigram_probs=False):\n",
    "    f = generate_word_sequence_recognition_wfst_with_silence(n, use_unigram_probs=use_unigram_probs)\n",
    "    f.set_input_symbols(state_table)\n",
    "    f.set_output_symbols(phone_table)\n",
    "    return f\n",
    "\n",
    "\n",
    "def create_bigram_lexical(n):\n",
    "    f = generate_bigram_wfst(n)\n",
    "    f.set_input_symbols(state_table)\n",
    "    f.set_output_symbols(phone_table)\n",
    "    return f\n",
    "\n",
    "\n",
    "def memory_of_wfst(f):\n",
    "    '''\n",
    "    Compute a measure of the memory required for your decoder by providing counts\n",
    "    of number of states and arcs in the WFST.\n",
    "    '''\n",
    "    all_states = []\n",
    "    all_arcs = []\n",
    "    for state in f.states():\n",
    "        all_states.append(state)\n",
    "        for arc in f.arcs(state):\n",
    "            all_arcs.append(arc)\n",
    "    return len(all_states), len(all_arcs)\n",
    "\n",
    "    \n",
    "def get_avg_wer(all_losses, verbose=False):\n",
    "    all_wer = []\n",
    "    for error_counts, word_count in all_losses:\n",
    "        all_wer.append(sum(error_counts) / word_count)\n",
    "    \n",
    "    if verbose :\n",
    "        print(f'The average WER is {np.mean(all_wer):.2%}')    \n",
    "    return np.mean(all_wer)\n",
    "\n",
    "\n",
    "def get_avg_effciency(efficancy_measures, verbose=False):\n",
    "    decoding_time = np.mean(efficancy_measures[0])\n",
    "    backtrace_time = np.mean(efficancy_measures[1])\n",
    "    number_of_computions = np.mean(efficancy_measures[2])\n",
    "    if verbose:\n",
    "        print(f'The average decoding time is {decoding_time:.2f} seconds')\n",
    "        print(f'The average backtrace time is {backtrace_time:.2f} seconds')\n",
    "        print(f'The average number of computations is {number_of_computions:.2f}')\n",
    "    return decoding_time, backtrace_time, number_of_computions\n",
    "\n",
    "\n",
    "def decoding_loop(f, train_set=True, train_split=0.5, use_pruning=False, determinized=False, verbose=False, prune_threshold= None, bigram = False, histogram_pruning_threshold = 0):\n",
    "    all_losses = []\n",
    "    decoding_time = []\n",
    "    backtrace_time = []\n",
    "    number_of_computations = []\n",
    "    all_files = glob.glob('/group/teaching/asr/labs/recordings/*.wav')\n",
    "    train_files = all_files[:(int(train_split*len(all_files)))]\n",
    "    test_files = all_files[(int(train_split*len(all_files))):]\n",
    "    \n",
    "    if train_set:\n",
    "        files= train_files\n",
    "    else:\n",
    "        files = test_files\n",
    "    \n",
    "    for wav_file in tqdm(files):    \n",
    "        decoder  = MyViterbiDecoder(f, wav_file, verbose=verbose, use_pruning=use_pruning, determinized=determinized, bigram=bigram, histogram_pruning_threshold=histogram_pruning_threshold)\n",
    "        if use_pruning and prune_threshold!=None:\n",
    "            decoder.prune_threshold = prune_threshold\n",
    "        decoder.decode()\n",
    "        (state_path, words) = decoder.backtrace()  \n",
    "        transcription = read_transcription(wav_file)\n",
    "        error_counts = wer.compute_alignment_errors(transcription, words)\n",
    "        word_count = len(transcription.split())\n",
    "\n",
    "        all_losses.append((error_counts, word_count))\n",
    "        decoding_time.append(decoder.decode_time)\n",
    "        backtrace_time.append(decoder.backtrace_time)\n",
    "        number_of_computations.append(decoder.number_of_computiations)\n",
    "        if verbose:\n",
    "            print(f'Transcription: {transcription} || Prediction: {words} || (nsub, ndel, nin) :{error_counts}')\n",
    "    \n",
    "    efficancy_measures = (decoding_time, backtrace_time, number_of_computations)\n",
    "    return all_losses, efficancy_measures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created all the wfst\n"
     ]
    }
   ],
   "source": [
    "lex = parse_lexicon('lexicon.txt')\n",
    "word_table, phone_table, state_table = generate_symbol_tables(lex)\n",
    "n = 3\n",
    "f = create_wfst(n)\n",
    "f_silence = create_wfst_with_silence(n)\n",
    "f_bigram = create_bigram_lexical(n)\n",
    "f_det = fst.determinize(f)\n",
    "f_silence_det = fst.determinize(f_silence)\n",
    "f_bigram_det = fst.determinize(f_bigram)\n",
    "f_det_min = f_det.copy()\n",
    "f_det_min.minimize(allow_nondet = True)\n",
    "f_silence_det_min = f_silence_det.copy()\n",
    "f_silence_det_min.minimize(allow_nondet = True)\n",
    "f_bigram_det_min = f_bigram_det.copy()\n",
    "f_bigram_det_min.minimize(allow_nondet = True)\n",
    "f_tuned = create_tuned_wfst(n)\n",
    "f_tuned_det = fst.determinize(f_tuned)\n",
    "f_tuned_det_min = f_tuned_det.copy()\n",
    "f_tuned_det_min.minimize(allow_nondet = True)\n",
    "print('Created all the wfst')\n",
    "\n",
    "def run_exp(exp_dict ,wfst_type, det, train_set = True ,train_split = 0.5, verbose=False, use_pruning=False, prune_threshold=None, histogram_pruning_threshold=0, minimized=False):\n",
    "    wfsts = {'baseline': f, 'silence': f_silence, 'bigram': f_bigram, 'tuned': f_tuned}\n",
    "    wfsts_det = {'baseline': f_det, 'silence': f_silence_det, 'bigram': f_bigram_det, 'tuned': f_tuned_det}\n",
    "    wfsts_det_min = {'baseline': f_det_min, 'silence': f_silence_det_min, 'bigram': f_bigram_det_min, 'tuned': f_tuned_det_min}\n",
    "\n",
    "    if minimized:\n",
    "        wfst = wfsts_det_min[wfst_type]\n",
    "    elif det:\n",
    "        wfst = wfsts_det[wfst_type]\n",
    "    else:\n",
    "        wfst = wfsts[wfst_type]\n",
    "    bigram = False\n",
    "    if wfst_type == 'bigram':\n",
    "        bigram = True\n",
    "    \n",
    "    exp_dict['type'] = wfst_type\n",
    "    exp_dict['det'] = det\n",
    "    exp_dict['all_states'], exp_dict['all_arcs'] = memory_of_wfst(wfst)\n",
    "\n",
    "    print(f'Type {wfst_type}, det = {det}, minimized = {minimized}')\n",
    "    print(f'All states: {exp_dict[\"all_states\"]}, all arcs: {exp_dict[\"all_arcs\"]}')\n",
    "\n",
    "\n",
    "    all_losses, efficancy_measures = decoding_loop(wfst, train_set=train_set, train_split=train_split, determinized=det, verbose=verbose, bigram=bigram, use_pruning=use_pruning, prune_threshold=prune_threshold, histogram_pruning_threshold=histogram_pruning_threshold)\n",
    "    avg_wer = get_avg_wer(all_losses, verbose=True)\n",
    "    m1,m2,m3 = get_avg_effciency(efficancy_measures, verbose=verbose)\n",
    "    exp_dict['loss'].append(all_losses)\n",
    "    exp_dict['efficancy'].append(efficancy_measures)\n",
    "    exp_dict['acc'].append(avg_wer)\n",
    "    exp_dict['m1'].append(m1)\n",
    "    exp_dict['m2'].append(m2)\n",
    "    exp_dict['m3'].append(m3)\n",
    "    print('\\n\\n\\n')\n",
    "    return exp_dict, wfst\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dict= {\n",
    "    'loss' : [],\n",
    "    'efficancy':[],\n",
    "    'acc': [],\n",
    "    'm1': [],\n",
    "    'm2': [],\n",
    "    'm3': [],\n",
    "    'all_states': [],\n",
    "    'all_arcs': [],\n",
    "    'det': False,\n",
    "    'type': 'baseline'   \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "prune_thresholds = [i*10 for i in range(1,21)]\n",
    "det = False\n",
    "wfst_type = 'tuned'\n",
    "verbose = False\n",
    "size = 0.5\n",
    "\n",
    "for prune_threshold in prune_thresholds:\n",
    "    print(f'Threshold = {prune_threshold}')\n",
    "    exp_dict, wfst = run_exp(exp_dict, wfst_type, det, verbose=verbose, train_split=size, use_pruning=True, histogram_pruning_threshold=prune_threshold)\n",
    "    exp_dict['WFST'] = wfst\n",
    "\n",
    "    file_name = f'exp_dict_tuned_histogram_{prune_threshold}.pkl'\n",
    "    with open(file_name, 'wb') as handler:\n",
    "        pickle.dump(exp_dict, handler)\n",
    "    print(f'saved to {file_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the experiment results\n",
    "print(f'FST type: {exp_dict[\"type\"]}')\n",
    "print(f'Determinized: {exp_dict[\"det\"]}')\n",
    "print(f'All states: {exp_dict[\"all_states\"]}, all arcs: {exp_dict[\"all_arcs\"]}')\n",
    "print(f'WER: {exp_dict[\"acc\"]}')\n",
    "print(f'M1 (avg. Decoding time): {exp_dict[\"m1\"]}')\n",
    "print(f'M2 (avg. Backtrace time): {exp_dict[\"m2\"]}')\n",
    "print(f'M3 (avg. Number of Computations): {exp_dict[\"m3\"]}')\n",
    "\n",
    "draw(wfst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use figure size (width, height) in inches\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# create a plot with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "# plot the first subplot\n",
    "ax1.plot(prune_thresholds, exp_dict['acc'], 'o-')\n",
    "ax1.set_title('WER')\n",
    "ax1.set_xlabel('Prune Threshold')\n",
    "ax1.set_ylabel('WER')\n",
    "\n",
    "# plot the second subplot\n",
    "ax2.plot(prune_thresholds, exp_dict['m1'], 'o-')\n",
    "ax2.set_title('Decoding Time')\n",
    "ax2.set_xlabel('Prune Threshold')\n",
    "ax2.set_ylabel('Decoding Time')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "8696358051874a88c1d2911df8bbed88795a658bdc69c38e8bd7d9488224210f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
