{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████████████████████████████▏     | 269/318 [00:00<00:00, 27026.32it/s]\n"
     ]
    }
   ],
   "source": [
    "### import os\n",
    "import wer\n",
    "import openfst_python as fst\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from vetrbi import MyViterbiDecoder\n",
    "from utils import parse_lexicon, generate_symbol_tables\n",
    "from hmm import generate_word_sequence_recognition_wfst\n",
    "from hmm import generate_word_sequence_recognition_wfst_with_silance\n",
    "from hmm import generate_bigram_wfst\n",
    "\n",
    "from utils import draw\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "def create_wfst():\n",
    "    f = generate_word_sequence_recognition_wfst(3)\n",
    "    f.set_input_symbols(state_table)\n",
    "    f.set_output_symbols(phone_table)\n",
    "    return f\n",
    "\n",
    "def create_wfst_with_silance():\n",
    "    f = generate_word_sequence_recognition_wfst_with_silance(3, use_unigram_probs=False)\n",
    "    f.set_input_symbols(state_table)\n",
    "    f.set_output_symbols(phone_table)\n",
    "    return f\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_transcription(wav_file):\n",
    "    \"\"\"\n",
    "    Get the transcription corresponding to wav_file.\n",
    "    \"\"\"\n",
    "    \n",
    "    transcription_file = os.path.splitext(wav_file)[0] + '.txt'\n",
    "    \n",
    "    with open(transcription_file, 'r') as f:\n",
    "        transcription = f.readline().strip()\n",
    "    \n",
    "    return transcription\n",
    "\n",
    "def memory_of_wfst(f):\n",
    "    '''\n",
    "    Compute a measure of the memory required for your decoder by providing counts\n",
    "    of number of states and arcs in the WFST.\n",
    "    '''\n",
    "    all_states = []\n",
    "    all_arcs = []\n",
    "    for state in f.states():\n",
    "        all_states.append(state)\n",
    "        for arc in f.arcs(state):\n",
    "            all_arcs.append(arc)\n",
    "    return len(all_states), len(all_arcs)\n",
    "    \n",
    "def get_avg_wer(all_losses, verbose=False):\n",
    "    all_wer = []\n",
    "    for error_counts, word_count in all_losses:\n",
    "        all_wer.append(sum(error_counts) / word_count)\n",
    "    \n",
    "    if verbose :\n",
    "        print(f'The average WER is {np.mean(all_wer):.2%}')    \n",
    "    return np.mean(all_wer)\n",
    "\n",
    "def get_avg_effciency(efficancy_measures, verbose=False):\n",
    "    decoding_time = np.mean(efficancy_measures[0])\n",
    "    backtrace_time = np.mean(efficancy_measures[1])\n",
    "    number_of_computions = np.mean(efficancy_measures[2])\n",
    "    if verbose:\n",
    "        print(f'The average decoding time is {decoding_time:.2f} seconds')\n",
    "        print(f'The average backtrace time is {backtrace_time:.2f} seconds')\n",
    "        print(f'The average number of computations is {number_of_computions:.2f}')\n",
    "    return decoding_time, backtrace_time, number_of_computions\n",
    "\n",
    "\n",
    "def decoding_loop(f, train_set=True, train_split=0.5, use_pruning=False, determinized=False, verbose=False, prune_threshold= None, bigram = False):\n",
    "    all_losses = []\n",
    "    decoding_time = []\n",
    "    backtrace_time = []\n",
    "    number_of_computations = []\n",
    "    all_files = glob.glob('/group/teaching/asr/labs/recordings/*.wav')\n",
    "    train_files = all_files[:(int(train_split*len(all_files)))]\n",
    "    test_files = all_files[(int(train_split*len(all_files))):]\n",
    "    \n",
    "    if train_set:\n",
    "        files= train_files\n",
    "    else:\n",
    "        files = test_files\n",
    "    \n",
    "    for wav_file in tqdm(files):    \n",
    "        decoder  = MyViterbiDecoder(f, wav_file, verbose=verbose, use_pruning=use_pruning, determinized=determinized, bigram=bigram)\n",
    "        if use_pruning and prune_threshold!=None:\n",
    "            decoder.prune_threshold = prune_threshold\n",
    "        decoder.decode()\n",
    "        (state_path, words) = decoder.backtrace()  \n",
    "        transcription = read_transcription(wav_file)\n",
    "        error_counts = wer.compute_alignment_errors(transcription, words)\n",
    "        word_count = len(transcription.split())\n",
    "\n",
    "        all_losses.append((error_counts, word_count))\n",
    "        decoding_time.append(decoder.decode_time)\n",
    "        backtrace_time.append(decoder.backtrace_time)\n",
    "        number_of_computations.append(decoder.number_of_computiations)\n",
    "        if verbose:\n",
    "            print(f'Transcription: {transcription} || Prediction: {words} || (nsub, ndel, nin) :{error_counts}')\n",
    "    \n",
    "    efficancy_measures = (decoding_time, backtrace_time, number_of_computations)\n",
    "    return all_losses, efficancy_measures\n",
    "\n",
    "\n",
    "lex = parse_lexicon('lexicon.txt')\n",
    "word_table, phone_table, state_table = generate_symbol_tables(lex)\n",
    "f = create_wfst()\n",
    "f_silence = create_wfst_with_silance()\n",
    "f_det = fst.determinize(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1\n",
      "det = True\n",
      "All states: 1915, all arcs: 4034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|████████▌                               | 34/159 [28:38<1:24:27, 40.54s/it]"
     ]
    }
   ],
   "source": [
    "exp_dict= {\n",
    "    'loss' : [],\n",
    "    'efficancy':[],\n",
    "    'acc': [],\n",
    "    'm1': [],\n",
    "    'm2': [],\n",
    "    'm3': [],\n",
    "    'all_states': [],\n",
    "    'all_arcs': [],\n",
    "    'det': False,\n",
    "    'n': 0   \n",
    "}\n",
    "\n",
    "\n",
    "def create_bigram_lexical(n):\n",
    "    f = generate_bigram_wfst(n)\n",
    "    f.set_input_symbols(state_table)\n",
    "    f.set_output_symbols(phone_table)\n",
    "    return f\n",
    "\n",
    "ns = [1,2,3]\n",
    "\n",
    "for n in ns:\n",
    "    print(f'N = {n}')\n",
    "    f_bigram = create_bigram_lexical(n)\n",
    "    exp_dict['n'] = n\n",
    "    det = True\n",
    "    if det:\n",
    "        f_bigram = fst.determinize(f_bigram)\n",
    "    exp_dict['det'] = det\n",
    "    exp_dict['all_states'], exp_dict['all_arcs'] = memory_of_wfst(f_bigram)\n",
    "    verbose = False\n",
    "    print(f'det = {det}')\n",
    "    print(f'All states: {exp_dict[\"all_states\"]}, all arcs: {exp_dict[\"all_arcs\"]}')\n",
    "    all_losses, efficancy_measures = decoding_loop(f_bigram, train_set=True, train_split=0.5, determinized=det, verbose=verbose, bigram=True)\n",
    "    avg_wer = get_avg_wer(all_losses, verbose=True)\n",
    "    m1,m2,m3 = get_avg_effciency(efficancy_measures, verbose=verbose)\n",
    "    exp_dict['loss'].append(all_losses)\n",
    "    exp_dict['efficancy'].append(efficancy_measures)\n",
    "    exp_dict['acc'].append(avg_wer)\n",
    "    exp_dict['m1'].append(m1)\n",
    "    exp_dict['m2'].append(m2)\n",
    "    exp_dict['m3'].append(m3)\n",
    "\n",
    "    print('\\n\\n\\n')\n",
    "    file_name = f'exp_dict_baseline_det_{det}_bigram_{n}.pkl'\n",
    "    with open(file_name, 'wb') as handler:\n",
    "        pickle.dump(exp_dict, handler)\n",
    "    print(f'saved to {file_name}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ======================================================"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "8696358051874a88c1d2911df8bbed88795a658bdc69c38e8bd7d9488224210f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
